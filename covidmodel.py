# -*- coding: utf-8 -*-
"""143518682_31005_Covid

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uxQY_MkQ-4syIZnJRWBXIDqQSqv3bMqu
"""

#from google.colab import drive
#drive.mount('/content/drive/')

"""EDA and Data preprocessing"""

import pandas as pd

#file_path = "/content/drive/MyDrive/31005/Covid Data.csv"

file_path = "C:/Users/64407/Desktop/新冠31005/Covid Data.csv"
df = pd.read_csv(file_path)
print(df.head())

#Display the data type of each attribute and the first 5 unique values, EDA
data_info = {}
for column in df.columns:
    data_info[column] = {
        'dtype': df[column].dtype,
        'unique_values': df[column].unique()[:5]
    }

data_info

#Convert string to date format
df['DATE_DIED'] = pd.to_datetime(df['DATE_DIED'], errors='coerce', format='%d/%m/%Y')

#For the "NaT" value (9999-99-99), convert it to a specific date (such as 1900-01-01)
special_date = pd.Timestamp('1900-01-01')
df['DATE_DIED'].fillna(special_date, inplace=True)

df['DATE_DIED'] = df['DATE_DIED'].apply(lambda x: 1 if x == special_date else 2)

#target
count_not_died_converted = (df['DATE_DIED'] == 1).sum()
count_died_converted = (df['DATE_DIED'] == 2).sum()

count_not_died_converted, count_died_converted
#alive&died

#Define classification attribute columns
categorical_columns = [col for col in df.columns if col not in ["DATE_DIED", "AGE"]]
unique_values_count = {column: df[column].nunique() for column in categorical_columns}
unique_values_count

import matplotlib.pyplot as plt
#Count the number of empty values

missing_values_count = {}
for column in df.columns:
    missing_values_count[column] = df[column].isin([97, 98, 99]).sum()

#Filter out non zero statistical results
non_zero_missing_values = {key: value for key, value in missing_values_count.items() if value != 0}

#Draw a bar chart
plt.figure(figsize=(10, 10))
plt.bar(non_zero_missing_values.keys(), non_zero_missing_values.values(), color='skyblue')
plt.xlabel('Attributes')
plt.ylabel('Number of Missing Values')
plt.title('Number of Missing Values by Attribute')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

non_zero_missing_values

#Delete the specified attribute because there are too many missing values
columns_to_remove = ['INTUBED', 'ICU', 'PREGNANT']

# If the columns exist in the dataframe, remove them
for col in columns_to_remove:
    if col in df.columns:
        df.drop(columns=col, inplace=True)

# Verify the columns have been removed
df.columns

#Divide patient types into 1 and 2
df['CLASIFFICATION_FINAL'] = df['CLASIFFICATION_FINAL'].apply(lambda x: 1 if x in [1, 2, 3] else 2)

import pandas as pd
import matplotlib.pyplot as plt

df = df.rename(columns={'DATE_DIED': 'death'})
classification_target = 'death'
numerical_columns = [col for col in df.columns if df[col].dtype != 'object' and col != classification_target]


for column in numerical_columns:
    plt.figure(figsize=[8, 3])
    # Mean
    plt.subplot(1, 2, 1)
    df.groupby(classification_target)[column].mean().plot(kind='bar', color=['blue', 'green'])
    plt.ylabel('Mean')
    plt.title('Mean of'+column+'by'+classification_target)
    plt.xticks(rotation=0)
    # Variance
    plt.subplot(1, 2, 2)
    df.groupby(classification_target)[column].var().plot(kind='bar', color=['blue', 'green'])
    plt.ylabel('Variance')
    plt.title('Variance of'+column+'by'+classification_target)
    plt.xticks(rotation=0)
    plt.tight_layout()
    plt.show()



mean_values = df.groupby('death').mean()
variance_values = df.groupby('death').var()

mean_diff = mean_values.loc[2] - mean_values.loc[1]
var_diff = variance_values.loc[2] - variance_values.loc[1]

features_to_remove = [feature for feature in mean_diff.index if abs(mean_diff[feature]) < 0.1 or abs(var_diff[feature]) < 0.1]

df_filtered = df.drop(columns=features_to_remove)

print("Features to remove:", features_to_remove)
print(df_filtered.head())

import matplotlib.pyplot as plt
# count null

missing_values_count = {}
for column in df.columns:
    missing_values_count[column] = df[column].isin([97, 98, 99]).sum()

# filter non-0
non_zero_missing_values = {key: value for key, value in missing_values_count.items() if value != 0}
plt.figure(figsize=(10, 6))
plt.bar(non_zero_missing_values.keys(), non_zero_missing_values.values(), color='skyblue')
plt.xlabel('Attributes')
plt.ylabel('Number of Missing Values')
plt.title('Number of Missing Values by Attribute')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

non_zero_missing_values

#data cleaning
df = df[(df.PNEUMONIA == 1) | (df.PNEUMONIA == 2)]
df = df[(df.DIABETES == 1) | (df.DIABETES == 2)]
df = df[(df.COPD == 1) | (df.COPD == 2)]
df = df[(df.ASTHMA == 1) | (df.ASTHMA == 2)]
df = df[(df.INMSUPR == 1) | (df.INMSUPR == 2)]
df = df[(df.HIPERTENSION == 1) | (df.HIPERTENSION == 2)]
df = df[(df.OTHER_DISEASE == 1) | (df.OTHER_DISEASE == 2)]
df = df[(df.CARDIOVASCULAR == 1) | (df.CARDIOVASCULAR == 2)]
df = df[(df.OBESITY == 1) | (df.OBESITY == 2)]
df = df[(df.RENAL_CHRONIC == 1) | (df.RENAL_CHRONIC == 2)]
df = df[(df.TOBACCO == 1) | (df.TOBACCO == 2)]
print("Shape of df :",df.shape)
df.head()

import seaborn as sns
plt.figure(figsize=(12,10))
sns.heatmap(df.corr(),annot=True, fmt=".2f")
plt.title("Correlation Between Features",fontsize=18,color="green");

"""Start training the LR model"""

from sklearn.model_selection import train_test_split

x = df.drop(columns="death")
y = df["death"]

train_x, test_x, train_y, test_y = train_test_split(x,y, test_size=0.3, random_state=42)
print("Train_x :",train_x.shape)
print("Test_x :",test_x.shape)
print("Train_y :",train_y.shape)
print("Test_y :",test_y.shape)

"""Start training the LR model"""

from sklearn.linear_model import LogisticRegression  #import the model
from sklearn.metrics import classification_report


logreg = LogisticRegression(
    penalty='l2',
    fit_intercept=True,
    solver='lbfgs',
    max_iter=1000,
)

logreg.fit(train_x, train_y)

y_pred = logreg.predict(test_x)

accuracy = logreg.score(test_x, test_y)

print("Logistic Regression Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report(test_y, y_pred))

"""We should pay attention to recall rate."""

from sklearn.metrics import confusion_matrix

sns.heatmap(confusion_matrix(test_y, logreg.predict(test_x)), annot=True, fmt=".0f")
plt.title("Logistic Regression Confusion Matrix",fontsize=18, color="red");



plt.figure(figsize=(4,4))
sns.countplot(data=df, x='death')
plt.title("Distribution of 'death' column")
plt.xlabel("death")
plt.ylabel("Count")
plt.show()
# COUNT TARGET
count_not_died_converted = (df['death'] == 1).sum()
count_died_converted = (df['death'] == 2).sum()

count_not_died_converted, count_died_converted

"""DT modeling"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report
clf_tree = DecisionTreeClassifier(
    criterion='entropy',
    splitter='best',
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=5,
    max_features=None,
    random_state=42,
    max_leaf_nodes=None,
    min_impurity_decrease=0.0,
)

clf_tree.fit(train_x, train_y)

y_pred_tree = clf_tree.predict(test_x)

print(classification_report(test_y, y_pred_tree))

import numpy as np

importances = clf_tree.feature_importances_

sorted_indices = np.argsort(importances)[::-1]
plt.figure(figsize=(10, 10))
plt.title("Feature Importances")
plt.barh(range(train_x.shape[1]), importances[sorted_indices], align="center")
plt.yticks(range(train_x.shape[1]), train_x.columns[sorted_indices])
plt.gca().invert_yaxis()
plt.show()

"""RF Training"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
rf = RandomForestClassifier(
    n_estimators=300,
    criterion='entropy',
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=5,
    max_features='sqrt',
    bootstrap=True,
    random_state=42,
    class_weight='balanced'
)

rf.fit(train_x, train_y)

y_pred_rf = rf.predict(test_x)

print(classification_report(test_y, y_pred_rf))

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
import matplotlib.pyplot as plt
import numpy as np

# provided by chatgpt
#Defining hyperparametric grids for random forests
param_grid = {
    'n_estimators': [300],
    'criterion': ['gini', 'entropy'],
    'max_depth': [10],
    'min_samples_split': [5, 10],
    'min_samples_leaf': [2, 5],
    'max_features': ['sqrt'],
    'bootstrap': [True],
    'class_weight': ['balanced']
}

rf = RandomForestClassifier(random_state=42)

#Using GridSearchCV for hyperparameter optimization of random forests
grid_search = GridSearchCV(rf, param_grid, cv=3, n_jobs=-1, verbose=1)

grid_search.fit(train_x, train_y)

print("Best Parameters:", grid_search.best_params_)

y_pred_rf = grid_search.predict(test_x)

print(classification_report(test_y, y_pred_rf))

feature_importances = grid_search.best_estimator_.feature_importances_
sorted_idx = feature_importances.argsort()
plt.figure(figsize=(10, 12))
plt.barh(train_x.columns[sorted_idx], feature_importances[sorted_idx])
plt.xlabel("Feature Importance")
plt.title("Feature Importance using Random Forest")
plt.show()

#Print the importance of each feature
print("Feature Importances:")

for index in sorted_idx:
    print(f"{train_x.columns[index]}: {feature_importances[index]}")

n = int(0.5 * len(feature_importances))
top_n_indices = np.argsort(feature_importances)[-n:]

train_x_selected = train_x.iloc[:, top_n_indices]
test_x_selected = test_x.iloc[:, top_n_indices]

grid_search.best_estimator_.fit(train_x_selected, train_y)

y_pred_rf_selected = grid_search.best_estimator_.predict(test_x_selected)

print("\nClassification Report using Top Features:")
print(classification_report(test_y, y_pred_rf_selected))

# Print the selected features
selected_features = train_x.columns[top_n_indices]
print("Selected Features:")
print(selected_features)



